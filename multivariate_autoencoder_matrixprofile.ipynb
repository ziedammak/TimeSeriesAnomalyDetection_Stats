{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLLab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_oTkMGBMh3_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from matplotlib import pyplot as plt\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Input, Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from keras.models import Model\n",
        "import seaborn as sns\n",
        "import stumpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoenc = pd.DataFrame(columns=['id', 'anomalies'])"
      ],
      "metadata": {
        "id": "2CLKOZqT8CNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = pathlib.Path('/content/phase1')\n",
        "for path in sorted(path.iterdir()):\n",
        "  if str(path)=='/content/phase1/.ipynb_checkpoints':\n",
        "    pass\n",
        "  else:\n",
        "    X = np.loadtxt(path)\n",
        "    df =pd.DataFrame({'value':X})\n",
        "    filename=str(path).split('\\\\')[-1]\n",
        "    print(filename)\n",
        "\n",
        "    point = int(filename.split('_')[-1].split('.')[0])\n",
        "    id = int(filename.split('_')[0].split('/')[-1])\n",
        "    #from univariate to multivariate data\n",
        "    df['date'] = [i for i in range(len(df))]\n",
        "    df['velocity'] = df['value'] - df['value'].shift(1)\n",
        "    df['acc'] = df['velocity'] - df['velocity'].shift(1)\n",
        "    ws=[10,15,40,50,100,150, 220, 300, 450, 500,600, 700]\n",
        "    for w in range(ws):\n",
        "      mps = []\n",
        "      mps = stumpy.stump(df['value'], w)\n",
        "      mps_ = [item[0] for item in mps]\n",
        "      df['mp_'+str(w)] = pd.Series(mps_)\n",
        "\n",
        "    df.dropna(inplace=True) \n",
        "    train, test = df[:point], df[point:]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(train[['value']])\n",
        "\n",
        "    train['value'] = scaler.transform(train[['value']])\n",
        "    test['value'] = scaler.transform(test[['value']])\n",
        "\n",
        "    seq_size = 100  # Number of time steps to look back \n",
        "    #Larger sequences (look further back) may improve forecasting.\n",
        "\n",
        "\n",
        "    def to_sequences(x, y, seq_size=1):\n",
        "        x_values = []\n",
        "        y_values = []\n",
        "\n",
        "        for i in range(len(x)-seq_size):\n",
        "            #print(i)\n",
        "            x_values.append(x.iloc[i:(i+seq_size)].values)\n",
        "            y_values.append(y.iloc[i+seq_size])\n",
        "            \n",
        "        return np.array(x_values), np.array(y_values)\n",
        "\n",
        "    trainX, trainY = to_sequences(train[['value']], train['value'], seq_size)\n",
        "    testX, testY = to_sequences(test[['value']], test['value'], seq_size)\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "    model.add(Dropout(rate=0.2))\n",
        "\n",
        "    model.add(RepeatVector(trainX.shape[1]))\n",
        "\n",
        "    model.add(LSTM(128, return_sequences=True))\n",
        "    model.add(Dropout(rate=0.2))\n",
        "    model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
        "    model.compile(optimizer='adam', loss='mae')\n",
        "    model.summary()\n",
        "\n",
        "    # fit model\n",
        "    history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
        "\n",
        "    plt.plot(history.history['loss'], label='Training loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    #model.evaluate(testX, testY)\n",
        "\n",
        "    trainPredict = model.predict(trainX)\n",
        "    trainMAE = np.mean(np.abs(trainPredict - trainX), axis=1)\n",
        "    plt.hist(trainMAE, bins=30)\n",
        "    max_trainMAE = 0.3 \n",
        "\n",
        "    testPredict = model.predict(testX)\n",
        "    testMAE = np.mean(np.abs(testPredict - testX), axis=1)\n",
        "    plt.hist(testMAE, bins=30)\n",
        "\n",
        "    #Capture all details in a DataFrame for easy plotting\n",
        "    anomaly_df = pd.DataFrame(test[seq_size:])\n",
        "    anomaly_df['testMAE'] = testMAE\n",
        "    anomaly_df['max_trainMAE'] = max_trainMAE\n",
        "    anomaly_df['anomaly'] = anomaly_df['testMAE'] > anomaly_df['max_trainMAE']\n",
        "    anomaly_df['value'] = test[seq_size:]['value']\n",
        "\n",
        "    anomalies = anomaly_df.loc[anomaly_df['anomaly'] == True]\n",
        "    autoenc= autoenc.append({'id':id, 'anomalies' : list(anomalies['date'])[0]},ignore_index=True)"
      ],
      "metadata": {
        "id": "B4zLonUxlskk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aBfYQcIzlkNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ZygdJspjS_Hi"
      }
    }
  ]
}