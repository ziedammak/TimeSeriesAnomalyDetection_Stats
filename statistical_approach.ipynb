{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook could be directly excuted in google colab. The location should fit to the path variable.\n",
        "This github project was our main reference in developing this pipeline https://github.com/intellygenta/KDDCup2021/blob/main/20210531/code.py\n",
        "We took time to understand the ensembling approach and integrate our own metrics"
      ],
      "metadata": {
        "id": "HNwGz6PKl7Ay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAE5J8OkTiem",
        "outputId": "8c9722e9-30f2-4bfd-9427-caf4c4c11b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Requirement already satisfied: stumpy in /usr/local/lib/python3.7/dist-packages (1.10.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from stumpy) (1.19.5)\n",
            "Requirement already satisfied: numba>=0.48 in /usr/local/lib/python3.7/dist-packages (from stumpy) (0.51.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from stumpy) (1.7.3)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy) (57.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install stumpy>=1.5\n",
        "!pip install stumpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFohHpZWUB92",
        "outputId": "ce8cfa3c-17d1-4b8f-c85f-dfd7ddb4612b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pathlib\n",
        "filename ='drive/MyDrive/phase_1/070_UCR_Anomaly_17555.txt'\n",
        "drive.mount('/content/drive/')\n",
        "path = pathlib.Path('/content/drive/MyDrive/phase_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG1eej3xTRHy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import pathlib\n",
        "import tqdm\n",
        "import stumpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nls6tDIITpOQ"
      },
      "outputs": [],
      "source": [
        "txt_dirpath = path  # Place the txt files in this directory\n",
        "\n",
        "# Parameter setting\n",
        "min_window_size = 40\n",
        "max_window_size = 800\n",
        "growth_rate = 1.1\n",
        "denom_threshold = 0.1\n",
        "upper_threshold = 0.75\n",
        "lower_threshold = 0.25\n",
        "const_threshold = 0.05\n",
        "min_coef = 0.5\n",
        "small_quantile = 0.1\n",
        "padding_length = 3\n",
        "train_length = 10\n",
        "use_gpu = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LEe5G61VWyx"
      },
      "outputs": [],
      "source": [
        "# Determine window sizes\n",
        "size = int(np.log(max_window_size / min_window_size) / np.log(growth_rate)) + 1\n",
        "rates = np.full(size, growth_rate) ** np.arange(size)\n",
        "ws = (min_window_size * rates).astype(int)\n",
        "\n",
        "# Select stump function\n",
        "if use_gpu:\n",
        "    stump = stumpy.gpu_stump\n",
        "else:\n",
        "    stump = stumpy.stump\n",
        "\n",
        "# Anomaly score names\n",
        "names = ['zscore',\n",
        "         'tukey1',\n",
        "         'tukey2',\n",
        "    'orig_p2p',\n",
        "    'diff_p2p',\n",
        "    'acc_p2p',\n",
        "    'orig_p2p_inv',\n",
        "    'diff_small',\n",
        "    'acc_std',\n",
        "    'acc_std_inv'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP_cCuVCsCbl",
        "outputId": "c37a47ac-4f9e-4e7b-ae22-e0ad1171163d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/phase_2')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt_dirpath"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score(X, number, split, w):\n",
        "        \n",
        "    # original data\n",
        "    seq = pd.DataFrame(X, columns=['orig'])\n",
        "\n",
        "    # velocity (diff) and acceleration (acc) \n",
        "    seq['diff'] = seq['orig'].diff(1)\n",
        "    seq['acc'] = seq['diff'].diff(1)    \n",
        "\n",
        "    #Interquartile: TUKEY\n",
        "    q1 = seq['diff'].rolling(window=w).quantile(lower_threshold)\n",
        "    q2 = seq['diff'].rolling(window=w).quantile(upper_threshold)\n",
        "    min = seq['diff'].rolling(window=w).min()\n",
        "    seq['tukey1'] = ((q1 - min) / (q2 - q1)).shift(-w)\n",
        "\n",
        "    q1 = seq['acc'].rolling(window=w).quantile(lower_threshold)\n",
        "    q2 = seq['acc'].rolling(window=w).quantile(upper_threshold)\n",
        "    min = seq['acc'].rolling(window=w).min()\n",
        "    seq['tukey2'] = ((q1 - min) / (q2 - q1)).shift(-w)\n",
        "\n",
        "\n",
        "    #Zscore\n",
        "    col_mean = seq['orig'].rolling(window=w).mean()\n",
        "    col_std = seq['orig'].rolling(window=w).std()\n",
        "    seq[\"zscore\"] = abs((seq[\"orig\"] - col_mean)/col_std).shift(-w)\n",
        "\n",
        "    # standard deviation (std)\n",
        "    for name in ['orig', 'acc']:\n",
        "        seq[f'{name}_std'] = seq[name].rolling(w).std().shift(-w)\n",
        "    \n",
        "    #  1st order and 2nd order amplitude\n",
        "    for name in ['orig', 'diff', 'acc']:\n",
        "        rolling_max = seq[name].rolling(w).max()\n",
        "        rolling_min = seq[name].rolling(w).min()\n",
        "        seq[f'{name}_p2p'] = (rolling_max - rolling_min).shift(-w)\n",
        "        print(seq)\n",
        "    \n",
        "    # Interquartile: diff small\n",
        "    diff_abs = seq['diff'].abs()\n",
        "    cond = diff_abs <= diff_abs.quantile(small_quantile)\n",
        "    seq['diff_small'] = cond.rolling(w).mean().shift(-w)\n",
        "    \n",
        "    # inverse (inv)\n",
        "    for name in ['orig_p2p', 'acc_std']:\n",
        "        numer = seq[name].mean()\n",
        "        denom = seq[name].clip(lower=numer * denom_threshold)\n",
        "        seq[f'{name}_inv'] = numer / denom\n",
        "    \n",
        "    # coef for penalizing subsequences with little change\n",
        "    name = 'orig_p2p'\n",
        "    mean = seq[name].mean()\n",
        "    upper = mean * upper_threshold\n",
        "    lower = mean * lower_threshold\n",
        "    const = mean * const_threshold\n",
        "    seq['coef'] = (seq[name] - lower) / (upper - lower)\n",
        "    seq['coef'].clip(upper=1.0, lower=0.0, inplace=True)\n",
        "    cond = (seq[name] <= const).rolling(2 * w).max().shift(-w) == 1\n",
        "    seq.loc[cond, 'coef'] = 0.0\n",
        "        \n",
        "\n",
        "    \n",
        "    # Smooth and mask anomaly score\n",
        "    padding = w * padding_length\n",
        "    seq['mask'] = 0.0\n",
        "    seq.loc[seq.index[w:-w-padding], 'mask'] = 1.0\n",
        "    seq['mask'] = seq['mask'].rolling(padding, min_periods=1).sum() / padding\n",
        "    for name in names:\n",
        "        seq[f'{name}_score'] = seq[name].rolling(w).mean() * seq['mask']\n",
        "    \n",
        "    return seq\n",
        "\n",
        "# Evaluate anomaly score for each time series\n",
        "results = []\n",
        "for txt_filepath in sorted(txt_dirpath.iterdir()):\n",
        "    \n",
        "    # Load time series\n",
        "    X = np.loadtxt(txt_filepath)\n",
        "    number = txt_filepath.stem.split('_')[0]\n",
        "    split = int(txt_filepath.stem.split('_')[-1])\n",
        "    #print(f'\\n{txt_filepath.name} {split}/{len(X)}', flush=True)\n",
        "    \n",
        "    # Evaluate anomaly score for each window size w\n",
        "    for w in tqdm.tqdm(ws):\n",
        "        \n",
        "        # Skip long subsequence\n",
        "        if w * train_length > split:\n",
        "            continue\n",
        "            \n",
        "        # Compute anomaly score\n",
        "        seq = compute_score(X, number, split, w)\n",
        "        \n",
        "        # Skip if coef is small\n",
        "        if seq['coef'].mean() < min_coef:\n",
        "            continue\n",
        "            \n",
        "        # Evaluate anomaly score\n",
        "        for name in names:\n",
        "            \n",
        "            # Copy anomaly score\n",
        "            y = seq[f'{name}_score'].copy()\n",
        "            print(y)\n",
        "            \n",
        "            # Find local maxima\n",
        "            cond = (y == y.rolling(w, center=True, min_periods=1).max())\n",
        "            y.loc[~cond] = np.nan\n",
        "            \n",
        "            # Find 1st peak\n",
        "            index1 = y.idxmax()\n",
        "            value1 = y.max()\n",
        "            \n",
        "            # Skip if all score is NaN\n",
        "            if not np.isfinite(value1):\n",
        "                continue\n",
        "                \n",
        "            # Skip if train data has 1st peak\n",
        "            begin = index1 - w\n",
        "            end = index1 + w\n",
        "            if begin < split:\n",
        "                continue\n",
        "\n",
        "            # Find 2nd peak\n",
        "            y.iloc[begin:end] = np.nan\n",
        "            index2 = y.idxmax()\n",
        "            value2 = y.max()\n",
        "            \n",
        "            # Skip if 2nd peak height is zero\n",
        "            if value2 == 0:\n",
        "                continue\n",
        "            \n",
        "            # Evaluate rate of 1st peak height to 2nd peak height\n",
        "            rate = value1 / value2\n",
        "            results.append([number, w, name, rate, begin, end, index1, value1, index2, value2])\n"
      ],
      "metadata": {
        "id": "T2laaW8GqnD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "results = pd.DataFrame(results, columns=['number', 'w', 'name', 'rate', 'begin', 'end', 'index1', 'value1', 'index2', 'value2'])\n",
        "submission.to_csv('/content/drive/MyDrive/result11.csv')\n",
        "# Make submission csv\n",
        "submission = results.loc[results.groupby('number')['rate'].idxmax(), 'index1']\n",
        "submission.index = np.arange(len(submission)) + 1\n",
        "submission.name = 'location'\n",
        "submission.index.name = 'No.'\n",
        "submission.to_csv('result.csv')"
      ],
      "metadata": {
        "id": "1p28DWbvkOFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcGFb_tV21EA"
      },
      "outputs": [],
      "source": [
        "submission1 = results.loc[results.groupby('number')['rate'].idxmax()]\n",
        "submission1=submission1[[\"number\",\"index1\",\"name\"]]\n",
        "submission1['number'] = submission1['number'].astype(int)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58RDNxZpJyYc"
      },
      "outputs": [],
      "source": [
        "submission1 = submission1.rename(columns={'name': 'anomalymethod'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUy_U88JVwVj"
      },
      "outputs": [],
      "source": [
        "results1 = pd.read_csv('drive/MyDrive/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mG112I80JVd"
      },
      "outputs": [],
      "source": [
        "df1=submission1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVIP_4fp0M76"
      },
      "outputs": [],
      "source": [
        "df2=results1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTt9kciFSPQv"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OY57BJV1Fy-"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_elJgeRp-qcT"
      },
      "outputs": [],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzUiiwnpvRfn"
      },
      "outputs": [],
      "source": [
        "df1 = df1.set_index('number')\n",
        "df1.index = pd.to_numeric(df1.index, errors='coerce')\n",
        "df2 = results1.set_index('data_id')\n",
        "eval = df1.join(df2).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx2xyCN9wAvB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5JGj5yZwLPg"
      },
      "outputs": [],
      "source": [
        "eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rng7JTKMLaqe"
      },
      "outputs": [],
      "source": [
        "df3=pd.DataFrame(columns=['name','anomalyfunction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mvww2LBwH7t"
      },
      "outputs": [],
      "source": [
        "for i in range (len(eval)):\n",
        "    if eval.loc[i,'anomaly_start'] <= eval.loc[i,'index1'] and eval.loc[i,'index1'] <= eval.loc[i,'anomaly_end']:\n",
        "        eval.loc[i,'algo_eval'] = True  \n",
        "        print(eval.loc[i,'name'],eval.loc[i,'anomalymethod'])\n",
        "\n",
        "\n",
        "    else:\n",
        "        eval.loc[i,'algo_eval'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpXZiPGlManA"
      },
      "outputs": [],
      "source": [
        "eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XSAVc9HMsYp"
      },
      "outputs": [],
      "source": [
        "rslt_df = eval[eval['algo_eval'] == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei2tvLYhL_gv"
      },
      "outputs": [],
      "source": [
        "rslt_df[\"anomalymethod\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHS1ygtDS1IG",
        "outputId": "35881edc-08b5-482e-d03d-221a5d854e2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True     117\n",
              "False     83\n",
              "Name: algo_eval, dtype: int64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval[\"algo_eval\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5rPUcDI4Xff"
      },
      "outputs": [],
      "source": [
        "eval.to_csv('./out.csv')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv5x8a6SvxQi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "imp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}